<!doctype html><html lang=en><head><meta charset=utf-8><title>AI or ain't: Neural Networks</title><meta name=description content="Neural network and deep learning introduction for those who skipped the math class but wants to follow the trend"><meta name=author content="Serge Zaitsev"><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><link rel="shortcut icon" href=data:,><link rel="shortcut icon" sizes=32x32 href=/favicon.png><link rel="shortcut icon" sizes=192x192 href=/favicon-192x192.png><link rel=apple-touch-icon href=/favicon-192x192.png><link rel=alternate type=application/rss+xml title=RSS href=/rss.xml><link rel=canonical href=https://zserge.com/posts/ai-nn/><meta property="og:title" content="AI or ain't: Neural Networks"><meta property="og:type" content="article"><meta property="og:url" content="https://zserge.com/posts/ai-nn/"><meta property="og:image" content="https://zserge.com/logo.png"><meta property="og:description" content="Neural network and deep learning introduction for those who skipped the math class but wants to follow the trend"><meta property="og:locale" content="en_US"><meta name=twitter:card content="Neural network and deep learning introduction for those who skipped the math class but wants to follow the trend"><meta name=twitter:site content="@zsergo"><style type=text/css>body{padding:1rem;margin:auto;max-width:46rem;font-family:pt serif,Georgia,Cambria,times new roman,Times,serif;line-height:1.5;font-size:20px;color:rgba(0,0,0,.87);-webkit-font-smoothing:antialiased;font-weight:300}nav{display:flex;justify-content:space-between;align-items:center;margin:1em 0 3em}nav ul,nav li{display:inline-block;list-style:none;margin:0 0 0 .5rem;padding:0}nav ul{margin-right:1rem}.logo{background-color:rgba(0,0,0,.87);color:#fff;min-width:48px;min-height:48px;font-size:28px;border-radius:2px;display:flex;justify-content:center;align-items:center}.logo:hover{color:#fff}h1,h2{line-height:1.2;font-family:roboto,system-ui,segoe ui,Helvetica,Arial,sans-serif;font-weight:400;text-transform:uppercase;margin:1.34em 0 0}h1{font-size:1.95em}h2{font-size:1.25em;border-bottom:2px solid rgba(0,0,0,.87)}h1 a{color:rgba(0,0,0,.87)}p{margin:.67em 0 1em}a{color:#0076df;text-decoration:none;word-break:break-word}a:hover{color:rgba(0,0,0,.87)}ul,ol{list-style-position:outside;margin-left:1em}img{display:block;margin-left:auto;margin-right:auto;max-width:100%}pre,code{font-family:roboto mono,SFMono-Regular,Consolas,liberation mono,Menlo,monospace;font-weight:400;font-size:18px;color:rgba(0,0,0,.6);background:#eee}code{padding:.2rem .4rem;font-size:14px;border-radius:2px}pre{overflow-y:auto;line-height:20px;border-radius:2px;padding:1em}pre code{font-size:14px;padding:0;color:rgba(0,0,0,.87)}footer{font-size:12px}@media(prefers-color-scheme:dark){.logo{color:#444;background-color:#e4e4e4}.logo:hover{color:#444}body,h1 a,h2 a{background-color:#444;color:#e4e4e4}pre,pre code{background:#333;color:#e4e4e4}h2{border-bottom:1px solid #e4e4e4}code{color:#aaa;background:#333}a{color:#e39777}a:hover{color:#e4e4e4}img{filter:grayscale(30%)}}.hl{display:block;width:100%;background-color:#ffc}.ow,.gh,.gp,.gs,.gu,.nt,.nn,.ne,.ni,.nc,.kr,.kn,.kd,.kc,.k{font-weight:700}.c,.ch,.cm,.c1,.cs,.ge{color:#777}</style><link rel="shortcut icon" href=/favicon.ico></head><body><header><nav><a class=logo href=/>Z</a><ul><li><a href=/about/>about</a></li><li><a href=/posts/>posts</a></li><li><a href=https://mastodon.social/@zserge rel=me>@me</a></li><li><a href=https://github.com/zserge rel=me>&lt;/>me</a></li></ul></nav></header><div id=content><h1>AI or ain't: Neural Networks</h1><p>Previously we explored early &ldquo;intelligent&rdquo; software based on <a href=/posts/ai-eliza/>rules</a> and <a href=/posts/ai-markov/>Markov chains</a>, but the results were not too convincing. Modern AI is all about neural networks, so let&rsquo;s fast-forward to the time when neural networks have been invented, which is&mldr; 1944? Correct, the idea of an artificial neuron dates many decades back, and it&rsquo;s a very simple one, really.</p><h2 id=neural-network-in-6-lines-of-code>neural network in 6 lines of code</h2><p>A neural network is nothing but a few multiplications and additions. Maybe with an occasional <code>if</code> to add some flavour.</p><p>Here&rsquo;s a bite-sized neural network in JavaScript – a single input layer, one output layer, and one hidden layer. It can predict the value of a binary XOR function using two numbers as inputs:</p><div class=highlight><pre class=chroma><code class=language-js data-lang=js><span class=kr>const</span> <span class=nx>relu</span> <span class=o>=</span> <span class=nx>x</span> <span class=p>=&gt;</span> <span class=nx>x</span> <span class=o>&lt;</span> <span class=mi>0</span> <span class=o>?</span> <span class=mi>0</span> <span class=o>:</span> <span class=nx>x</span><span class=p>;</span>
<span class=c1>// Run the network for the given input array X
</span><span class=c1></span><span class=kr>const</span> <span class=nx>nn</span> <span class=o>=</span> <span class=nx>X</span> <span class=p>=&gt;</span> <span class=p>{</span>
  <span class=c1>// Define the parameters for the network
</span><span class=c1></span>  <span class=kr>const</span> <span class=nx>W1</span> <span class=o>=</span> <span class=p>[[</span><span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=o>-</span><span class=mi>1</span><span class=p>],</span> <span class=p>[</span><span class=o>-</span><span class=mf>0.7</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.7</span><span class=p>]],</span> <span class=nx>W2</span> <span class=o>=</span> <span class=p>[</span><span class=o>-</span><span class=mf>1.8</span><span class=p>,</span> <span class=mf>1.3</span><span class=p>];</span>
  <span class=kr>const</span> <span class=nx>b1</span> <span class=o>=</span> <span class=p>[</span><span class=mi>1</span><span class=p>,</span> <span class=mf>1.3</span><span class=p>],</span> <span class=nx>b2</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span>
  <span class=c1>// Calculate the output
</span><span class=c1></span>  <span class=kr>const</span> <span class=nx>hidden</span> <span class=o>=</span> <span class=nx>b1</span><span class=p>.</span><span class=nx>map</span><span class=p>((</span><span class=nx>b</span><span class=p>,</span> <span class=nx>i</span><span class=p>)</span> <span class=p>=&gt;</span> <span class=nx>relu</span><span class=p>(</span><span class=nx>X</span><span class=p>.</span><span class=nx>reduce</span><span class=p>((</span><span class=nx>a</span><span class=p>,</span> <span class=nx>x</span><span class=p>,</span> <span class=nx>j</span><span class=p>)</span> <span class=p>=&gt;</span> <span class=nx>a</span><span class=o>+</span><span class=nx>x</span><span class=o>*</span><span class=nx>W1</span><span class=p>[</span><span class=nx>i</span><span class=p>][</span><span class=nx>j</span><span class=p>],</span> <span class=mi>0</span><span class=p>)</span> <span class=o>+</span> <span class=nx>b</span><span class=p>))</span>
  <span class=k>return</span> <span class=nx>relu</span><span class=p>(</span><span class=nx>hidden</span><span class=p>.</span><span class=nx>reduce</span><span class=p>((</span><span class=nx>a</span><span class=p>,</span> <span class=nx>x</span><span class=p>,</span> <span class=nx>i</span><span class=p>)</span> <span class=p>=&gt;</span> <span class=nx>a</span><span class=o>+</span><span class=nx>x</span><span class=o>*</span><span class=nx>W2</span><span class=p>[</span><span class=nx>i</span><span class=p>],</span> <span class=mi>0</span><span class=p>)</span> <span class=o>+</span> <span class=nx>b2</span><span class=p>);</span>
<span class=p>};</span>

<span class=c1>// Predict XOR function:
</span><span class=c1></span><span class=nx>console</span><span class=p>.</span><span class=nx>log</span><span class=p>(</span><span class=nx>nn</span><span class=p>([</span><span class=mi>0</span><span class=p>,</span> <span class=mi>0</span><span class=p>]));</span> <span class=c1>// 0^0 =&gt; 0
</span><span class=c1></span><span class=nx>console</span><span class=p>.</span><span class=nx>log</span><span class=p>(</span><span class=nx>nn</span><span class=p>([</span><span class=mi>0</span><span class=p>,</span> <span class=mi>1</span><span class=p>]));</span> <span class=c1>// 0^1 =&gt; 0.9916
</span><span class=c1></span><span class=nx>console</span><span class=p>.</span><span class=nx>log</span><span class=p>(</span><span class=nx>nn</span><span class=p>([</span><span class=mi>1</span><span class=p>,</span> <span class=mi>0</span><span class=p>]));</span> <span class=c1>// 1^0 =&gt; 0.9916
</span><span class=c1></span><span class=nx>console</span><span class=p>.</span><span class=nx>log</span><span class=p>(</span><span class=nx>nn</span><span class=p>([</span><span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>]));</span> <span class=c1>// 1^1 =&gt; 0
</span></code></pre></div><p>In six lines of code we have an &ldquo;AI&rdquo;. Maybe not the flashiest in town, but it gets the job done.</p><p>Now, for the uninitiated, things might seem a bit muddled. What&rsquo;s with this &ldquo;relu&rdquo;, what are <em>W1</em>, <em>W2</em>, <em>b1</em>, <em>b2</em> and why do we do these map/reduces?</p><p><em>W</em> are the weight arrays. All inputs in a network are multiplied by some weight coefficients, then summed up. Optionally, a &ldquo;bias&rdquo; number (<em>b</em>) is added to that.</p><p><a href=https://en.wikipedia.org/wiki/Rectifier_(neural_networks)>ReLU</a> is a &ldquo;rectified linear unit&rdquo;, but practically it&rsquo;s just a <code>max(x, 0)</code> to add some non-linearity to the network. Without it our network would stuck with linear functions like <code>ax+b</code>, no matter how you shuffle the things. But ReLU function makes a &ldquo;broken line&rdquo; and for negative numbers returns zero. Combining linear functions with ReLU and growing the size of W/b arrays could suddenly result in very complex shapes and functions and solve really complex problems.</p><p>Functions like ReLU are known as &ldquo;activation functions&rdquo;. You could swap ReLU for sin(), tanh(), exp(), Gaussian, but we&rsquo;ll keep it straightforward and stick with ReLU.</p><p>Now there&rsquo;s one last bit of magic left in those 6 lines of code implementing a network. Who gets to decide the values for W and b?</p><h2 id=training-a-network>Training a network</h2><p>This is where neural networks shine. They can be automatically trained to pick the right weights to solve the given problem. Humans only need to feed them inputs and expected outputs, and give them some time to grok it.</p><p>For each iteration in the training phase a network predicts an output, compares with with the expected one, calculates the difference and adjusts the weights.</p><p>Basically, we have a rather long chain of functions like <code>relu(X*W+b)</code> and our network tries to pick the lock by finding the right W and b to match the expected results.</p><p>Imagine a guessing game. We have a formula <code>X+b</code>. I picked some <code>b</code> that you have to guess. For every guess of yours I will be naming some <code>X</code> and some expected output of <code>X+b</code>. You would have to correct the value of <code>b</code> according to my clues.</p><p>Say, your guess is <code>b=5</code>. I say: actually, for <code>X=b</code> I expected the output to be 17, while with your <code>b=5</code> the output is 13. Now you know you are off by 4 and should increase your <code>b</code> by 4 as well, getting <code>b=5+4</code> or <code>b=9</code>.</p><p>Now, we have a formula <code>X•W</code>. You&rsquo;ve picked some random <code>W=3</code>. I say that for <code>X=5</code> the result should be not 15 but 25. Now you know that you are off by 10, and since your guess for X was 5 - you need to adjust W by 10÷5=2, making your next guess W=5 the right one.</p><p>Such adjustments are known as gradients. These are very basic examples, but as our network becomes larger and the formulas become more complex finding the right gradients becomes not so trivial.</p><h2 id=autograd>Autograd</h2><p>Some machine learning libraries have a concept of <a href=https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html>autograd</a>, automatic differential calculator for gradients. How does it work?</p><p>Let&rsquo;s start with a scalar type for all our numbers:</p><div class=highlight><pre class=chroma><code class=language-js data-lang=js><span class=kr>const</span> <span class=nx>Scalar</span> <span class=o>=</span> <span class=p>(</span><span class=nx>val</span><span class=p>)</span> <span class=p>=&gt;</span> <span class=p>{</span>
  <span class=kd>let</span> <span class=nx>x</span> <span class=o>=</span> <span class=p>{</span>
    <span class=nx>val</span><span class=p>,</span>
    <span class=nx>add</span><span class=o>:</span> <span class=nx>y</span> <span class=p>=&gt;</span> <span class=nx>Scalar</span><span class=p>(</span><span class=nx>x</span><span class=p>.</span><span class=nx>val</span> <span class=o>+</span> <span class=nx>y</span><span class=p>.</span><span class=nx>val</span><span class=p>),</span>
    <span class=nx>mul</span><span class=o>:</span> <span class=nx>y</span> <span class=p>=&gt;</span> <span class=nx>Scalar</span><span class=p>(</span><span class=nx>x</span><span class=p>.</span><span class=nx>val</span> <span class=o>*</span> <span class=nx>y</span><span class=p>.</span><span class=nx>val</span><span class=p>),</span>
  <span class=p>};</span>
  <span class=k>return</span> <span class=nx>x</span><span class=p>;</span>
<span class=p>};</span>

<span class=kr>const</span> <span class=nx>a</span> <span class=o>=</span> <span class=nx>Scalar</span><span class=p>(</span><span class=mi>5</span><span class=p>),</span> <span class=nx>b</span> <span class=o>=</span> <span class=nx>Scalar</span><span class=p>(</span><span class=mi>3</span><span class=p>),</span> <span class=nx>c</span> <span class=o>=</span> <span class=nx>Scalar</span><span class=p>(</span><span class=mi>2</span><span class=p>);</span>
<span class=kr>const</span> <span class=nx>d</span> <span class=o>=</span> <span class=nx>a</span><span class=p>.</span><span class=nx>add</span><span class=p>(</span><span class=nx>b</span><span class=p>).</span><span class=nx>mul</span><span class=p>(</span><span class=nx>c</span><span class=p>);</span> <span class=c1>// d.val == 16
</span></code></pre></div><p>So far it&rsquo;s only used to abstract away all arithmetic operations. Most ML frameworks do that to build a graph of operations and execute that graph later on GPU or somewhere in the cloud.</p><p>As we build the operation graph we can additionally store references to other scalars (that were inputs to this operation) and how much they would impact the result of this operation (gradients):</p><div class=highlight><pre class=chroma><code class=language-js data-lang=js><span class=kr>const</span> <span class=nx>Scalar</span> <span class=o>=</span> <span class=p>(</span><span class=nx>val</span><span class=p>,</span> <span class=nx>refs</span> <span class=o>=</span> <span class=p>[],</span> <span class=nx>gradrefs</span> <span class=o>=</span> <span class=p>[])</span> <span class=p>=&gt;</span> <span class=p>{</span>
  <span class=kd>let</span> <span class=nx>x</span> <span class=o>=</span> <span class=p>{</span>
    <span class=nx>val</span><span class=p>,</span> <span class=nx>refs</span><span class=p>,</span> <span class=nx>gradrefs</span><span class=p>,</span> <span class=nx>grad</span><span class=o>:</span> <span class=mi>0</span><span class=p>,</span>
    <span class=nx>add</span><span class=o>:</span> <span class=p>(</span><span class=nx>y</span><span class=p>)</span> <span class=p>=&gt;</span> <span class=nx>Scalar</span><span class=p>(</span><span class=nx>x</span><span class=p>.</span><span class=nx>val</span> <span class=o>+</span> <span class=nx>y</span><span class=p>.</span><span class=nx>val</span><span class=p>,</span> <span class=p>[</span><span class=nx>x</span><span class=p>,</span> <span class=nx>y</span><span class=p>],</span> <span class=p>[</span><span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>]),</span>
    <span class=nx>mul</span><span class=o>:</span> <span class=p>(</span><span class=nx>y</span><span class=p>)</span> <span class=p>=&gt;</span> <span class=nx>Scalar</span><span class=p>(</span><span class=nx>x</span><span class=p>.</span><span class=nx>val</span> <span class=o>*</span> <span class=nx>y</span><span class=p>.</span><span class=nx>val</span><span class=p>,</span> <span class=p>[</span><span class=nx>x</span><span class=p>,</span> <span class=nx>y</span><span class=p>],</span> <span class=p>[</span><span class=nx>y</span><span class=p>.</span><span class=nx>val</span><span class=p>,</span> <span class=nx>x</span><span class=p>.</span><span class=nx>val</span><span class=p>]),</span>
  <span class=p>};</span>
  <span class=k>return</span> <span class=nx>x</span><span class=p>;</span>
<span class=p>};</span>
</code></pre></div><p>From the guessing game above we&rsquo;ve learned that for addition operation we should adjust the parameter linearly (e.g. we missed the target by 10, we adjust the parameter by 10). But for the multiplication it depends on the multiplier value (e.g. if we are off by 12 and the other multiplier was 4 &ndash; we should adjust by 12/4=3). We can add more operations: division is inverted to multiplication and subtraction is inverted to addition:</p><div class=highlight><pre class=chroma><code class=language-js data-lang=js><span class=kr>const</span> <span class=nx>Scalar</span> <span class=o>=</span> <span class=p>...</span> <span class=p>{</span>
    <span class=nx>sub</span><span class=o>:</span> <span class=p>(</span><span class=nx>y</span><span class=p>)</span> <span class=p>=&gt;</span> <span class=nx>Scalar</span><span class=p>(</span><span class=nx>x</span><span class=p>.</span><span class=nx>val</span> <span class=o>-</span> <span class=nx>y</span><span class=p>.</span><span class=nx>val</span><span class=p>,</span> <span class=p>[</span><span class=nx>x</span><span class=p>,</span> <span class=nx>y</span><span class=p>],</span> <span class=p>[</span><span class=mi>1</span><span class=p>,</span> <span class=o>-</span><span class=mi>1</span><span class=p>]),</span>
    <span class=nx>div</span><span class=o>:</span> <span class=p>(</span><span class=nx>y</span><span class=p>)</span> <span class=p>=&gt;</span> <span class=nx>Scalar</span><span class=p>(</span><span class=nx>x</span><span class=p>.</span><span class=nx>val</span> <span class=o>/</span> <span class=nx>y</span><span class=p>.</span><span class=nx>val</span><span class=p>,</span> <span class=p>[</span><span class=nx>x</span><span class=p>,</span> <span class=nx>y</span><span class=p>],</span> <span class=p>[</span><span class=mi>1</span><span class=o>/</span><span class=nx>y</span><span class=p>.</span><span class=nx>val</span><span class=p>,</span> <span class=o>-</span><span class=nx>x</span><span class=p>.</span><span class=nx>val</span><span class=o>/</span><span class=p>(</span><span class=nx>y</span><span class=p>.</span><span class=nx>val</span><span class=o>*</span><span class=nx>y</span><span class=p>.</span><span class=nx>val</span><span class=p>)]),</span>
    <span class=nx>pow</span><span class=o>:</span> <span class=p>(</span><span class=nx>n</span><span class=p>)</span> <span class=p>=&gt;</span> <span class=nx>Scalar</span><span class=p>(</span><span class=nb>Math</span><span class=p>.</span><span class=nx>pow</span><span class=p>(</span><span class=nx>x</span><span class=p>.</span><span class=nx>val</span><span class=p>,</span> <span class=nx>n</span><span class=p>),</span> <span class=p>[</span><span class=nx>x</span><span class=p>],</span> <span class=p>[</span><span class=nx>n</span><span class=o>*</span><span class=nb>Math</span><span class=p>.</span><span class=nx>pow</span><span class=p>(</span><span class=nx>x</span><span class=p>.</span><span class=nx>val</span><span class=p>,</span> <span class=nx>n</span><span class=o>-</span><span class=mi>1</span><span class=p>)]),</span>
    <span class=nx>relu</span><span class=o>:</span> <span class=p>()</span> <span class=p>=&gt;</span> <span class=nx>Scalar</span><span class=p>(</span><span class=nb>Math</span><span class=p>.</span><span class=nx>max</span><span class=p>(</span><span class=nx>x</span><span class=p>.</span><span class=nx>val</span><span class=p>,</span> <span class=mi>0</span><span class=p>),</span> <span class=p>[</span><span class=nx>x</span><span class=p>],</span> <span class=p>[</span><span class=o>+</span><span class=p>(</span><span class=nx>x</span><span class=p>.</span><span class=nx>val</span> <span class=o>&gt;</span> <span class=mi>0</span><span class=p>)]),</span>
    <span class=p>...</span>
<span class=p>}</span>
</code></pre></div><p>Note that for <code>pow()</code> we only support constant N as an input (not a scalar), to simplify the derivative calculation.</p><p>Now, having a sufficiently complex graph of operations - how to we propagate the gradient value among them? We need to iterate recursively from the result of the operation graph back to its inputs, processing each node once and only once. We can have a flag inside Scalar, or we can put all nodes into some ordered set as we scan the graph to avoid repetitions:</p><div class=highlight><pre class=chroma><code class=language-js data-lang=js><span class=kr>const</span> <span class=nx>Scalar</span> <span class=o>=</span> <span class=p>...</span> <span class=p>{</span>
    <span class=p>...</span>
    <span class=nx>backward</span><span class=o>:</span> <span class=p>()</span> <span class=p>=&gt;</span> <span class=p>{</span>
      <span class=kr>const</span> <span class=nx>visited</span> <span class=o>=</span> <span class=p>[],</span> <span class=nx>nodes</span> <span class=o>=</span> <span class=p>[];</span>
      <span class=kr>const</span> <span class=nx>buildGraph</span> <span class=o>=</span> <span class=p>(</span><span class=nx>n</span><span class=p>)</span> <span class=p>=&gt;</span> <span class=p>{</span>
        <span class=k>if</span> <span class=p>(</span><span class=nx>visited</span><span class=p>.</span><span class=nx>indexOf</span><span class=p>(</span><span class=nx>n</span><span class=p>)</span> <span class=o>&lt;</span> <span class=mi>0</span><span class=p>)</span> <span class=p>{</span>
          <span class=nx>visited</span><span class=p>.</span><span class=nx>push</span><span class=p>(</span><span class=nx>n</span><span class=p>);</span>
          <span class=nx>n</span><span class=p>.</span><span class=nx>refs</span><span class=p>.</span><span class=nx>forEach</span><span class=p>(</span><span class=nx>buildGraph</span><span class=p>)</span>
          <span class=nx>nodes</span><span class=p>.</span><span class=nx>push</span><span class=p>(</span><span class=nx>n</span><span class=p>);</span>
        <span class=p>}</span>
      <span class=p>};</span>
      <span class=nx>buildGraph</span><span class=p>(</span><span class=nx>x</span><span class=p>);</span>
      <span class=nx>x</span><span class=p>.</span><span class=nx>grad</span> <span class=o>=</span> <span class=mi>1</span><span class=p>;</span>
      <span class=nx>nodes</span><span class=p>.</span><span class=nx>reverse</span><span class=p>();</span>
      <span class=nx>nodes</span><span class=p>.</span><span class=nx>forEach</span><span class=p>(</span><span class=nx>n</span> <span class=p>=&gt;</span> <span class=nx>n</span><span class=p>.</span><span class=nx>refs</span><span class=p>.</span><span class=nx>forEach</span><span class=p>((</span><span class=nx>ref</span><span class=p>,</span> <span class=nx>i</span><span class=p>)</span> <span class=p>=&gt;</span>
        <span class=nx>ref</span><span class=p>.</span><span class=nx>grad</span> <span class=o>+=</span> <span class=nx>n</span><span class=p>.</span><span class=nx>grad</span> <span class=o>*</span> <span class=nx>n</span><span class=p>.</span><span class=nx>gradrefs</span><span class=p>[</span><span class=nx>i</span><span class=p>]));</span>
    <span class=p>}</span>
<span class=p>}</span>
</code></pre></div><p>In this way the value of <code>grad=1</code> is propagated back to the leaves of the operation tree, to the very inputs. Resulting gradients there would indicate how much influence each of the input has on the resulting value of the formula, allowing us to adjust those parameters accordingly.</p><h2 id=building-a-smarter-network>Building a smarter network</h2><p>Having a Scalar we can use it as a basis to our multi-layer network that would be able to train itself.</p><p>Probably worth mentioning that real ML frameworks use &ldquo;tensors&rdquo; instead scalars to perform operations on complete arrays and matrices, using most of the processing power. Also they tend to use operator overloading to hide this asynchronous operation graph behind the scenes. We, on the other hand, are building a very slow, inefficient, but a very explicit and simple network.</p><p>The smallest element of a neural network is a neuron. It&rsquo;s a virtual unit that in our case performs a single <code>relu(W*X+b)</code> operation, where <em>W</em> is an array of weights, <em>X</em> is an input array and <em>b</em> is bias value.</p><div class=highlight><pre class=chroma><code class=language-js data-lang=js><span class=kr>const</span> <span class=nx>Neuron</span> <span class=o>=</span> <span class=p>(</span><span class=nx>numInputs</span><span class=p>)</span> <span class=p>=&gt;</span> <span class=p>{</span>
  <span class=kd>let</span> <span class=nx>n</span> <span class=o>=</span> <span class=p>{</span>
    <span class=nx>w</span><span class=o>:</span> <span class=nb>Array</span><span class=p>(</span><span class=nx>numInputs</span><span class=p>).</span><span class=nx>fill</span><span class=p>(</span><span class=mi>0</span><span class=p>).</span><span class=nx>map</span><span class=p>((</span><span class=nx>_</span><span class=p>,</span> <span class=nx>i</span><span class=p>)</span> <span class=p>=&gt;</span> <span class=nx>Scalar</span><span class=p>(</span><span class=nb>Math</span><span class=p>.</span><span class=nx>random</span><span class=p>()</span><span class=o>*</span><span class=mi>2</span><span class=o>-</span><span class=mi>1</span><span class=p>)),</span>
    <span class=nx>b</span><span class=o>:</span> <span class=nx>Scalar</span><span class=p>(</span><span class=mi>0</span><span class=p>),</span>
    <span class=nx>params</span><span class=o>:</span> <span class=p>()</span> <span class=p>=&gt;</span> <span class=p>[...</span><span class=nx>n</span><span class=p>.</span><span class=nx>w</span><span class=p>,</span> <span class=nx>n</span><span class=p>.</span><span class=nx>b</span><span class=p>],</span>
    <span class=nb>eval</span><span class=o>:</span> <span class=p>(</span><span class=nx>x</span><span class=p>)</span> <span class=p>=&gt;</span> <span class=nx>x</span><span class=p>.</span><span class=nx>map</span><span class=p>((</span><span class=nx>xi</span><span class=p>,</span> <span class=nx>i</span><span class=p>)</span> <span class=p>=&gt;</span> <span class=nx>xi</span><span class=p>.</span><span class=nx>mul</span><span class=p>(</span><span class=nx>n</span><span class=p>.</span><span class=nx>w</span><span class=p>[</span><span class=nx>i</span><span class=p>])).</span><span class=nx>reduce</span><span class=p>((</span><span class=nx>a</span><span class=p>,</span> <span class=nx>xwi</span><span class=p>)</span> <span class=p>=&gt;</span> <span class=nx>a</span><span class=p>.</span><span class=nx>add</span><span class=p>(</span><span class=nx>xwi</span><span class=p>),</span> <span class=nx>n</span><span class=p>.</span><span class=nx>b</span><span class=p>).</span><span class=nx>relu</span><span class=p>(),</span>
  <span class=p>};</span>
  <span class=k>return</span> <span class=nx>n</span><span class=p>;</span>
<span class=p>};</span>
</code></pre></div><p>Neurons are combined into layers. The most common layer type of a network is &ldquo;dense&rdquo; layer. This means all outputs from the previous layer serve as neuron inputs for all the neurons in the current layer. Here <code>nout</code> is the number of layer outputs (equals to number of neurons in the layer) and <code>nin</code> is the number of inputs of each neuron in this layer:</p><div class=highlight><pre class=chroma><code class=language-js data-lang=js><span class=kr>const</span> <span class=nx>Layer</span> <span class=o>=</span> <span class=p>(</span><span class=nx>nin</span><span class=p>,</span> <span class=nx>nout</span><span class=p>)</span> <span class=p>=&gt;</span> <span class=p>{</span>
  <span class=kd>let</span> <span class=nx>l</span> <span class=o>=</span> <span class=p>{</span>
    <span class=nx>neurons</span><span class=o>:</span> <span class=nb>Array</span><span class=p>(</span><span class=nx>nout</span><span class=p>).</span><span class=nx>fill</span><span class=p>(</span><span class=mi>0</span><span class=p>).</span><span class=nx>map</span><span class=p>(()</span> <span class=p>=&gt;</span> <span class=nx>Neuron</span><span class=p>(</span><span class=nx>nin</span><span class=p>)),</span>
    <span class=nx>params</span><span class=o>:</span> <span class=p>()</span> <span class=p>=&gt;</span> <span class=nx>l</span><span class=p>.</span><span class=nx>neurons</span><span class=p>.</span><span class=nx>reduce</span><span class=p>((</span><span class=nx>p</span><span class=p>,</span> <span class=nx>n</span><span class=p>)</span> <span class=p>=&gt;</span> <span class=p>[...</span><span class=nx>p</span><span class=p>,</span> <span class=p>...</span><span class=nx>n</span><span class=p>.</span><span class=nx>params</span><span class=p>()],</span> <span class=p>[]),</span>
    <span class=nb>eval</span><span class=o>:</span> <span class=nx>x</span> <span class=p>=&gt;</span> <span class=nx>l</span><span class=p>.</span><span class=nx>neurons</span><span class=p>.</span><span class=nx>map</span><span class=p>(</span><span class=nx>n</span> <span class=p>=&gt;</span> <span class=nx>n</span><span class=p>.</span><span class=nb>eval</span><span class=p>(</span><span class=nx>x</span><span class=p>)),</span>
  <span class=p>};</span>
  <span class=k>return</span> <span class=nx>l</span><span class=p>;</span>
<span class=p>};</span>
</code></pre></div><p>Finally, the network (or &ldquo;multi-layer perceptron&rdquo;) is an array of dense layers, where outputs from one layer are passed as inputs to the following one:</p><div class=highlight><pre class=chroma><code class=language-js data-lang=js><span class=kr>const</span> <span class=nx>MLP</span> <span class=o>=</span> <span class=p>(</span><span class=nx>N</span><span class=p>)</span> <span class=p>=&gt;</span> <span class=p>{</span>
  <span class=kd>let</span> <span class=nx>mlp</span> <span class=o>=</span> <span class=p>{</span>
    <span class=nx>layers</span><span class=o>:</span> <span class=nx>N</span><span class=p>.</span><span class=nx>slice</span><span class=p>(</span><span class=mi>1</span><span class=p>).</span><span class=nx>map</span><span class=p>((</span><span class=nx>n</span><span class=p>,</span> <span class=nx>i</span><span class=p>)</span> <span class=p>=&gt;</span> <span class=nx>Layer</span><span class=p>(</span><span class=nx>N</span><span class=p>[</span><span class=nx>i</span><span class=p>],</span> <span class=nx>N</span><span class=p>[</span><span class=nx>i</span><span class=o>+</span><span class=mi>1</span><span class=p>],</span> <span class=nx>i</span> <span class=o>&lt;</span> <span class=nx>N</span><span class=p>.</span><span class=nx>length</span> <span class=o>-</span> <span class=mi>1</span><span class=p>)),</span>
    <span class=nb>eval</span><span class=o>:</span> <span class=nx>x</span> <span class=p>=&gt;</span> <span class=nx>mlp</span><span class=p>.</span><span class=nx>layers</span><span class=p>.</span><span class=nx>reduce</span><span class=p>((</span><span class=nx>a</span><span class=p>,</span> <span class=nx>l</span><span class=p>)</span> <span class=p>=&gt;</span> <span class=nx>a</span> <span class=o>=</span> <span class=nx>l</span><span class=p>.</span><span class=nb>eval</span><span class=p>(</span><span class=nx>a</span><span class=p>),</span> <span class=nx>x</span><span class=p>),</span>
    <span class=nx>params</span><span class=o>:</span> <span class=p>()</span> <span class=p>=&gt;</span> <span class=nx>mlp</span><span class=p>.</span><span class=nx>layers</span><span class=p>.</span><span class=nx>reduce</span><span class=p>((</span><span class=nx>p</span><span class=p>,</span> <span class=nx>l</span><span class=p>)</span> <span class=p>=&gt;</span> <span class=p>[...</span><span class=nx>p</span><span class=p>,</span> <span class=p>...</span><span class=nx>l</span><span class=p>.</span><span class=nx>params</span><span class=p>()],</span> <span class=p>[]),</span>
    <span class=nx>train</span><span class=o>:</span> <span class=p>(</span><span class=nx>Xset</span><span class=p>,</span> <span class=nx>Yset</span><span class=p>,</span> <span class=nx>rate</span> <span class=o>=</span> <span class=mf>0.1</span><span class=p>)</span> <span class=p>=&gt;</span> <span class=p>{</span>
      <span class=kr>const</span> <span class=nx>out</span> <span class=o>=</span> <span class=nx>Xset</span><span class=p>.</span><span class=nx>map</span><span class=p>(</span><span class=nx>xs</span> <span class=p>=&gt;</span> <span class=nx>mlp</span><span class=p>.</span><span class=nb>eval</span><span class=p>(</span><span class=nx>xs</span><span class=p>.</span><span class=nx>map</span><span class=p>(</span><span class=nx>x</span> <span class=p>=&gt;</span> <span class=nx>Scalar</span><span class=p>(</span><span class=nx>x</span><span class=p>))));</span>
      <span class=kr>const</span> <span class=nx>err</span> <span class=o>=</span> <span class=nx>Yset</span><span class=p>.</span><span class=nx>map</span><span class=p>((</span><span class=nx>ys</span><span class=p>,</span> <span class=nx>i</span><span class=p>)</span> <span class=p>=&gt;</span> <span class=nx>ys</span><span class=p>.</span><span class=nx>map</span><span class=p>((</span><span class=nx>ysi</span><span class=p>,</span> <span class=nx>j</span><span class=p>)</span> <span class=p>=&gt;</span> <span class=nx>Scalar</span><span class=p>(</span><span class=nx>ysi</span><span class=p>).</span><span class=nx>sub</span><span class=p>(</span><span class=nx>out</span><span class=p>[</span><span class=nx>i</span><span class=p>][</span><span class=nx>j</span><span class=p>]).</span><span class=nx>pow</span><span class=p>(</span><span class=mi>2</span><span class=p>)));</span>
      <span class=kr>const</span> <span class=nx>totalErr</span> <span class=o>=</span> <span class=nx>err</span><span class=p>.</span><span class=nx>reduce</span><span class=p>((</span><span class=nx>e</span><span class=p>,</span> <span class=nx>es</span><span class=p>)</span> <span class=p>=&gt;</span> <span class=nx>e</span><span class=p>.</span><span class=nx>add</span><span class=p>(</span><span class=nx>es</span><span class=p>.</span><span class=nx>reduce</span><span class=p>((</span><span class=nx>a</span><span class=p>,</span> <span class=nx>e</span><span class=p>)</span> <span class=p>=&gt;</span> <span class=nx>a</span><span class=p>.</span><span class=nx>add</span><span class=p>(</span><span class=nx>e</span><span class=p>),</span> <span class=nx>Scalar</span><span class=p>(</span><span class=mi>0</span><span class=p>))),</span> <span class=nx>Scalar</span><span class=p>(</span><span class=mi>0</span><span class=p>)).</span><span class=nx>div</span><span class=p>(</span><span class=nx>Scalar</span><span class=p>(</span><span class=nx>err</span><span class=p>.</span><span class=nx>length</span><span class=p>));</span>
      <span class=nx>mlp</span><span class=p>.</span><span class=nx>params</span><span class=p>().</span><span class=nx>map</span><span class=p>(</span><span class=nx>p</span> <span class=p>=&gt;</span> <span class=nx>p</span><span class=p>.</span><span class=nx>grad</span> <span class=o>=</span> <span class=mi>0</span><span class=p>);</span>
      <span class=nx>totalErr</span><span class=p>.</span><span class=nx>backward</span><span class=p>();</span>
      <span class=nx>mlp</span><span class=p>.</span><span class=nx>params</span><span class=p>().</span><span class=nx>map</span><span class=p>(</span><span class=nx>p</span> <span class=p>=&gt;</span> <span class=nx>p</span><span class=p>.</span><span class=nx>val</span> <span class=o>-=</span> <span class=nx>p</span><span class=p>.</span><span class=nx>grad</span> <span class=o>*</span> <span class=nx>rate</span><span class=p>);</span>
      <span class=k>return</span> <span class=nx>totalErr</span><span class=p>;</span>
    <span class=p>},</span>
  <span class=p>};</span>
  <span class=k>return</span> <span class=nx>mlp</span><span class=p>;</span>
<span class=p>};</span>
</code></pre></div><p>The most exciting part here is the &ldquo;train&rdquo; method. It takes an array of inputs (Xset) and an array of expected outputs (Yset). It calculates the actual outputs (out) by evaluation every input vector from Xset. Then it calculates the total error <code>∑(out[i]-Y[i])^2</code> between the expected output and the actual output.</p><p>Total error is the final result of the calculation graph of our network. So we reset the network gradients and run the autograd <code>backward()</code> method to propagate the gradients back to each and every parameter according to its influence.</p><p>Finally, we adjust those parameters in the loop. Note that we don&rsquo;t adjust them by the actual gradient. Instead we use a much smaller portion of it, defined by the learning rate value. The higher is the learning rate - the more radical are the changes in the network. This may sometimes cause it missing the goal. Smaller steps of course take more time, but are usually more efficient.</p><p>Can we teach it to solve the XOR function?</p><div class=highlight><pre class=chroma><code class=language-js data-lang=js><span class=kr>const</span> <span class=nx>X</span> <span class=o>=</span> <span class=p>[[</span><span class=mi>0</span><span class=p>,</span> <span class=mi>0</span><span class=p>],</span> <span class=p>[</span><span class=mi>0</span><span class=p>,</span> <span class=mi>1</span><span class=p>],</span> <span class=p>[</span><span class=mi>1</span><span class=p>,</span> <span class=mi>0</span><span class=p>],</span> <span class=p>[</span><span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>]];</span>
<span class=kr>const</span> <span class=nx>Y</span> <span class=o>=</span> <span class=p>[[</span><span class=mi>0</span><span class=p>],</span> <span class=p>[</span><span class=mi>1</span><span class=p>],</span> <span class=p>[</span><span class=mi>1</span><span class=p>],</span> <span class=p>[</span><span class=mi>0</span><span class=p>]];</span>
<span class=k>for</span> <span class=p>(</span><span class=kd>let</span> <span class=nx>i</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span> <span class=nx>i</span> <span class=o>&lt;</span> <span class=mi>100</span><span class=p>;</span> <span class=nx>i</span><span class=o>++</span><span class=p>)</span> <span class=p>{</span>
    <span class=nx>nn</span><span class=p>.</span><span class=nx>train</span><span class=p>(</span><span class=nx>X</span><span class=p>,</span> <span class=nx>Y</span><span class=p>,</span> <span class=mf>0.05</span><span class=p>);</span>
<span class=p>}</span>
<span class=nx>nn</span><span class=p>.</span><span class=nx>params</span><span class=p>().</span><span class=nx>map</span><span class=p>(</span><span class=nx>p</span> <span class=p>=&gt;</span> <span class=nx>console</span><span class=p>.</span><span class=nx>log</span><span class=p>(</span><span class=nx>p</span><span class=p>.</span><span class=nx>val</span><span class=p>));</span>
<span class=c1>// Params: 1, 1, -1, 1, 1, 0, -1.8, 0.9, 0
</span></code></pre></div><p>These params are not even close to the ones we used at the beginning, but nevertheless - the new network predicts a XOR function just fine.</p><p>What about more complex problems? There is a well-known &ldquo;two moon&rdquo; problem, where 2D points that are shaped like two almost-overlapping moons needed to be classified. A network needs to learn how to tell by the point parameters (x,y) which moon does it belong to.</p><p>Test data for the two moons problem can be generated like this:</p><div class=highlight><pre class=chroma><code class=language-js data-lang=js><span class=c1>// Test data generator for the &#34;two moons&#34; problem
</span><span class=c1></span><span class=kr>const</span> <span class=nx>moons</span> <span class=o>=</span> <span class=p>(</span><span class=nx>n</span><span class=p>)</span> <span class=p>=&gt;</span> <span class=p>{</span>
	<span class=kr>const</span> <span class=nx>unif</span> <span class=o>=</span> <span class=p>(</span><span class=nx>low</span><span class=p>,</span> <span class=nx>high</span><span class=p>)</span> <span class=p>=&gt;</span> <span class=nb>Math</span><span class=p>.</span><span class=nx>random</span><span class=p>()</span> <span class=o>*</span> <span class=p>(</span><span class=nx>high</span> <span class=o>-</span> <span class=nx>low</span><span class=p>)</span> <span class=o>+</span> <span class=nx>low</span><span class=p>;</span>
  <span class=kr>const</span> <span class=nx>data</span> <span class=o>=</span> <span class=p>[],</span> <span class=nx>labels</span> <span class=o>=</span> <span class=p>[];</span>
  <span class=k>for</span> <span class=p>(</span><span class=kd>let</span> <span class=nx>i</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span> <span class=nx>i</span> <span class=o>&lt;</span> <span class=nb>Math</span><span class=p>.</span><span class=nx>PI</span><span class=p>;</span> <span class=nx>i</span> <span class=o>+=</span> <span class=p>(</span><span class=nb>Math</span><span class=p>.</span><span class=nx>PI</span> <span class=o>*</span> <span class=mi>2</span><span class=p>)</span> <span class=o>/</span> <span class=nx>n</span><span class=p>)</span> <span class=p>{</span>
    <span class=nx>data</span><span class=p>.</span><span class=nx>push</span><span class=p>([</span><span class=nb>Math</span><span class=p>.</span><span class=nx>cos</span><span class=p>(</span><span class=nx>i</span><span class=p>)</span> <span class=o>+</span> <span class=nx>unif</span><span class=p>(</span><span class=o>-</span><span class=mf>0.1</span><span class=p>,</span> <span class=mf>0.1</span><span class=p>)</span> <span class=o>-</span> <span class=mf>0.5</span><span class=p>,</span> <span class=nb>Math</span><span class=p>.</span><span class=nx>sin</span><span class=p>(</span><span class=nx>i</span><span class=p>)</span> <span class=o>+</span> <span class=nx>unif</span><span class=p>(</span><span class=o>-</span><span class=mf>0.1</span><span class=p>,</span> <span class=mf>0.1</span><span class=p>)</span> <span class=o>-</span> <span class=mf>0.3</span><span class=p>]);</span>
    <span class=nx>labels</span><span class=p>.</span><span class=nx>push</span><span class=p>([</span><span class=mi>0</span><span class=p>]);</span>
		<span class=nx>data</span><span class=p>.</span><span class=nx>push</span><span class=p>([</span><span class=mf>0.5</span> <span class=o>-</span> <span class=nb>Math</span><span class=p>.</span><span class=nx>cos</span><span class=p>(</span><span class=nx>i</span><span class=p>)</span> <span class=o>+</span> <span class=nx>unif</span><span class=p>(</span><span class=o>-</span><span class=mf>0.1</span><span class=p>,</span> <span class=mf>0.1</span><span class=p>),</span> <span class=mf>0.2</span> <span class=o>-</span> <span class=nb>Math</span><span class=p>.</span><span class=nx>sin</span><span class=p>(</span><span class=nx>i</span><span class=p>)</span> <span class=o>+</span> <span class=nx>unif</span><span class=p>(</span><span class=o>-</span><span class=mf>0.1</span><span class=p>,</span> <span class=mf>0.1</span><span class=p>)]);</span>
		<span class=nx>labels</span><span class=p>.</span><span class=nx>push</span><span class=p>([</span><span class=mi>1</span><span class=p>]);</span>
  <span class=p>}</span>
  <span class=k>return</span> <span class=p>[</span><span class=nx>data</span><span class=p>,</span> <span class=nx>labels</span><span class=p>];</span>
<span class=p>}</span>

<span class=kr>const</span> <span class=p>[</span><span class=nx>X</span><span class=p>,</span> <span class=nx>Y</span><span class=p>]</span> <span class=o>=</span> <span class=nx>moons</span><span class=p>(</span><span class=mi>200</span><span class=p>);</span>
</code></pre></div><p>We can now define the structure of our network and train it:</p><div class=highlight><pre class=chroma><code class=language-js data-lang=js><span class=kr>const</span> <span class=nx>nn</span> <span class=o>=</span> <span class=nx>MLP</span><span class=p>([</span><span class=mi>2</span><span class=p>,</span><span class=mi>8</span><span class=p>,</span><span class=mi>4</span><span class=p>,</span><span class=mi>1</span><span class=p>])</span>

<span class=k>for</span> <span class=p>(</span><span class=kd>let</span> <span class=nx>i</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span> <span class=nx>i</span> <span class=o>&lt;</span> <span class=mi>100</span><span class=p>;</span> <span class=nx>i</span><span class=o>++</span><span class=p>)</span> <span class=p>{</span>
	<span class=kr>const</span> <span class=nx>e</span> <span class=o>=</span> <span class=nx>nn</span><span class=p>.</span><span class=nx>loss</span><span class=p>(</span><span class=nx>X</span><span class=p>,</span> <span class=nx>Y</span><span class=p>,</span> <span class=mf>0.2</span><span class=p>);</span>
	<span class=nx>console</span><span class=p>.</span><span class=nx>log</span><span class=p>(</span><span class=nx>i</span><span class=p>,</span> <span class=nx>e</span><span class=p>.</span><span class=nx>val</span><span class=p>);</span>
<span class=p>}</span>
</code></pre></div><p>In the first few iterations the error drops rapidly to only ~5% and then the network keeps reducing the error slowly and steadily. Finally, after 100 iterations it manages to classify the points belonging to the two moons perfectly:</p><p><img src=/images/nn/moons.png alt=moons></p><p>Full code is available on <a href=https://github.com/zserge/aint/tree/main/nn>GitHub</a>.</p><p>Of course training a real practical network is much harder. It&rsquo;s a very slow process, that would probably require a GPU or a <a href=https://en.wikipedia.org/wiki/Tensor_Processing_Unit>TPU</a>. You would need a proper framework like PyTorch or Tensorflow or Keras (although even a <a href=https://github.com/tinygrad/tinygrad>tinygrad</a> works surprisingly well for small cases and is so much easier to learn and understand). You would have to be aware of proper weights initialisation, optimisers, managing learning rate, regulations, dropout layers, batching and many other topics that turn machine learning into its own branch of science.</p><h2 id=ai>AI?</h2><p>But even our toy neural network despite its slowness is a real machine leaning algorithm. Is it intelligent though? It can&rsquo;t speak yet, but it can be trained automatically and gain knowledge about the problems it never seen before. What if instead of numbers such a network could take words as an input and produce sentences as an output?</p><p>Next part: <a href=/posts/ai-llm>Large Language Models</a>.</p><p>I hope you’ve enjoyed this article. You can follow – and contribute to – on <a href=https://github.com/zserge>Github</a>, <a href=https://mastodon.social/@zserge>Mastodon</a>, <a href=https://twitter.com/zsergo>Twitter</a> or subscribe via <a href=/rss.xml>rss</a>.</p><p class=date style=text-align:right><em>Jan 03, 2024</em></p><p>See also:
<a href=/posts/ai-markov/>AI or ain't: Markov Chains</a> and <a href=/posts/>more</a>.</p></div><footer><p>&copy;2012&ndash;2023 &#183;
<a class=h-card rel=me href=https://zserge.com/>Serge Zaitsev</a> &#183;
<a href=mailto:hello@zserge.com rel=me>hello@zserge.com</a> &#183;
<a href=https://mastodon.social/@zserge rel=me>@zserge@mastodon.social</a></p></footer><script>new Image().src='https://nullitics.com/file.gif?u='+encodeURI(location.href)+'&r='+encodeURI(document.referrer)+'&d='+screen.width</script><noscript><img src=https://nullitics.com/file.gif></noscript></body></html>