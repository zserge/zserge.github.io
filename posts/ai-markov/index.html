<!doctype html><html lang=en><head><meta charset=utf-8><title>AI or ain't: Markov Chains</title><meta name=description content="Exploring the world of Markov chains, learning how they predict text patterns and make a basic implementation that talks nonsense like Homer Simpson."><meta name=author content="Serge Zaitsev"><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><link rel="shortcut icon" href=data:,><link rel="shortcut icon" sizes=32x32 href=/favicon.png><link rel="shortcut icon" sizes=192x192 href=/favicon-192x192.png><link rel=apple-touch-icon href=/favicon-192x192.png><link rel=alternate type=application/rss+xml title=RSS href=/rss.xml><link rel=canonical href=https://zserge.com/posts/ai-markov/><meta property="og:title" content="AI or ain't: Markov Chains"><meta property="og:type" content="article"><meta property="og:url" content="https://zserge.com/posts/ai-markov/"><meta property="og:image" content="https://zserge.com/logo.png"><meta property="og:description" content="Exploring the world of Markov chains, learning how they predict text patterns and make a basic implementation that talks nonsense like Homer Simpson."><meta property="og:locale" content="en_US"><meta name=twitter:card content="Exploring the world of Markov chains, learning how they predict text patterns and make a basic implementation that talks nonsense like Homer Simpson."><meta name=twitter:site content="@zsergo"><style type=text/css>body{padding:1rem;margin:auto;max-width:46rem;font-family:pt serif,Georgia,Cambria,times new roman,Times,serif;line-height:1.5;font-size:20px;color:rgba(0,0,0,.87);-webkit-font-smoothing:antialiased;font-weight:300}nav{display:flex;justify-content:space-between;align-items:center;margin:1em 0 3em}nav ul,nav li{display:inline-block;list-style:none;margin:0 0 0 .5rem;padding:0}nav ul{margin-right:1rem}.logo{background-color:rgba(0,0,0,.87);color:#fff;min-width:48px;min-height:48px;font-size:28px;border-radius:2px;display:flex;justify-content:center;align-items:center}.logo:hover{color:#fff}h1,h2{line-height:1.2;font-family:roboto,system-ui,segoe ui,Helvetica,Arial,sans-serif;font-weight:400;text-transform:uppercase;margin:1.34em 0 0}h1{font-size:1.95em}h2{font-size:1.25em;border-bottom:2px solid rgba(0,0,0,.87)}h1 a{color:rgba(0,0,0,.87)}p{margin:.67em 0 1em}a{color:#0076df;text-decoration:none;word-break:break-word}a:hover{color:rgba(0,0,0,.87)}ul,ol{list-style-position:outside;margin-left:1em}img{display:block;margin-left:auto;margin-right:auto;max-width:100%}pre,code{font-family:roboto mono,SFMono-Regular,Consolas,liberation mono,Menlo,monospace;font-weight:400;font-size:18px;color:rgba(0,0,0,.6);background:#eee}code{padding:.2rem .4rem;font-size:14px;border-radius:2px}pre{overflow-y:auto;line-height:20px;border-radius:2px;padding:1em}pre code{font-size:14px;padding:0;color:rgba(0,0,0,.87)}footer{font-size:12px}@media(prefers-color-scheme:dark){.logo{color:#444;background-color:#e4e4e4}.logo:hover{color:#444}body,h1 a,h2 a{background-color:#444;color:#e4e4e4}pre,pre code{background:#333;color:#e4e4e4}h2{border-bottom:1px solid #e4e4e4}code{color:#aaa;background:#333}a{color:#e39777}a:hover{color:#e4e4e4}img{filter:grayscale(30%)}}.hl{display:block;width:100%;background-color:#ffc}.ow,.gh,.gp,.gs,.gu,.nt,.nn,.ne,.ni,.nc,.kr,.kn,.kd,.kc,.k{font-weight:700}.c,.ch,.cm,.c1,.cs,.ge{color:#777}</style><link rel="shortcut icon" href=/favicon.ico></head><body><header><nav><a class=logo href=/>Z</a><ul><li><a href=/about/>about</a></li><li><a href=/posts/>posts</a></li><li><a href=https://mastodon.social/@zserge rel=me>@me</a></li><li><a href=https://github.com/zserge rel=me>&lt;/>me</a></li></ul></nav></header><div id=content><h1>AI or ain't: Markov Chains</h1><p><a href=/posts/ai-eliza/>Eliza</a> was an impressive chatbot of its era. But all its success was hidden in a carefully crafted set of rules that made it sound like a therapist. <a href=https://en.wikipedia.org/wiki/Markov_chain>Markov chains</a> are more flexible, as they don&rsquo;t rely on strict guidelines, but rather use patterns and statistics to generate responses. Let&rsquo;s explore how Markov chains work and how to build one yourself.</p><h2 id=markov-chains>Markov chains</h2><p>A Markov chain is a mathematical model that describes a sequence of events where the outcome of each event depends solely on the state of the system at the previous event. They are often used to generate text based on the probability of transitioning from one word to another.</p><p>One of the most well-known examples of this technique is <a href=https://en.wikipedia.org/wiki/Mark_V._Shaney>Mark V. Shaney</a>, a bot that posted various messages in newsgroups in the late 1980s. Created by Rob Pike et al, it was trained on a corpus from Tao Te Ching (classic Taoist texts) and produced messages <a href=https://groups.google.com/g/net.singles/c/-l2tNE4L0IA/m/WP5y76K5MQUJ>like this one</a>:</p><div class=highlight><pre class=chroma><code class=language-txt data-lang=txt>People, having a much larger number of varieties, and are very
different from what one can find in Chinatowns accross the country
(things like pork buns, steamed dumplings, etc.) They can be cheap,
being sold for around 30 to 75 cents apiece (depending on size), are
generally not greasy, can be adequately explained by stupidity.
...
So I will conclude by saying that I can well understand that she might
soon have the time, it makes sense, again, to get the gist of my
argument, I was in that (though it&#39;s a Republican administration).
</code></pre></div><p>It sounds very much like English and with a certain degree of imagination might even make sense. Obviously, after some cherry-picking one may create a fairly meaningful text generated by a Markov chain.</p><h2 id=building-a-markov-chain>Building a Markov chain</h2><p>Believe it or not, a typical implementation of a Markov chain is around 30 lines of code including both, a training part and a text generator.</p><p>We start with training. Reading the training text line by line (assuming that each line is a sentence of its own) we split that into tokens (words) much like we did in <a href=/posts/ai-eliza>Eliza</a>, and start processing those tokens in order.</p><p>For the given prefix word we should store the following word in the chain. Also it would be convenient to put the first word in the chain into a possible sentence starter list (otherwise generated text may start mid-sentence and give away the fact that our chain is not that intelligent at all):</p><div class=highlight><pre class=chroma><code class=language-go data-lang=go><span class=kd>func</span> <span class=nf>main</span><span class=p>()</span> <span class=p>{</span>
  <span class=nx>chain</span> <span class=o>:=</span> <span class=kd>map</span><span class=p>[</span><span class=kt>string</span><span class=p>][]</span><span class=kt>string</span><span class=p>{}</span>
  <span class=nx>starts</span> <span class=o>:=</span> <span class=p>[]</span><span class=kt>string</span><span class=p>{}</span>
  <span class=nx>scanner</span> <span class=o>:=</span> <span class=nx>bufio</span><span class=p>.</span><span class=nf>NewScanner</span><span class=p>(</span><span class=nx>os</span><span class=p>.</span><span class=nx>Stdin</span><span class=p>)</span>
  <span class=k>for</span> <span class=nx>scanner</span><span class=p>.</span><span class=nf>Scan</span><span class=p>()</span> <span class=p>{</span>
    <span class=nx>words</span> <span class=o>:=</span> <span class=nx>strings</span><span class=p>.</span><span class=nf>Fields</span><span class=p>(</span><span class=nx>scanner</span><span class=p>.</span><span class=nf>Text</span><span class=p>())</span>
    <span class=k>if</span> <span class=nb>len</span><span class=p>(</span><span class=nx>words</span><span class=p>)</span> <span class=p>&lt;</span> <span class=mi>1</span> <span class=p>{</span>
      <span class=k>continue</span>
    <span class=p>}</span>
    <span class=nx>starts</span> <span class=p>=</span> <span class=nb>append</span><span class=p>(</span><span class=nx>starts</span><span class=p>,</span> <span class=nx>words</span><span class=p>[</span><span class=mi>0</span><span class=p>])</span>
    <span class=k>for</span> <span class=nx>i</span> <span class=o>:=</span> <span class=mi>0</span><span class=p>;</span> <span class=nx>i</span> <span class=p>&lt;</span> <span class=nb>len</span><span class=p>(</span><span class=nx>words</span><span class=p>)</span><span class=o>-</span><span class=mi>1</span><span class=p>;</span> <span class=nx>i</span><span class=o>++</span> <span class=p>{</span>
      <span class=nx>chain</span><span class=p>[</span><span class=nx>words</span><span class=p>[</span><span class=nx>i</span><span class=p>]]</span> <span class=p>=</span> <span class=nb>append</span><span class=p>(</span><span class=nx>chain</span><span class=p>[</span><span class=nx>words</span><span class=p>[</span><span class=nx>i</span><span class=p>]],</span> <span class=nx>words</span><span class=p>[</span><span class=nx>i</span><span class=o>+</span><span class=mi>1</span><span class=p>])</span>
    <span class=p>}</span>
  <span class=p>}</span>
  <span class=nx>fmt</span><span class=p>.</span><span class=nf>Println</span><span class=p>(</span><span class=nx>starts</span><span class=p>)</span>
  <span class=k>for</span> <span class=nx>k</span><span class=p>,</span> <span class=nx>v</span> <span class=o>:=</span> <span class=k>range</span> <span class=nx>chain</span> <span class=p>{</span>
    <span class=nx>fmt</span><span class=p>.</span><span class=nf>Println</span><span class=p>(</span><span class=nx>k</span><span class=p>,</span> <span class=nx>v</span><span class=p>)</span>
  <span class=p>}</span>
<span class=p>}</span>

<span class=c1>// Let&#39;s use the following text from Margaret Atwood as an input:
</span><span class=c1>//
</span><span class=c1>// She&#39;s too young, it&#39;s too late, we come apart, my arms are held, and the
</span><span class=c1>// edges go dark and nothing is left but a little window, a very little window,
</span><span class=c1>// like the wrong end of a telescope, like the window on a Christmas card, an old
</span><span class=c1>// one, night and ice outside, and within a candle, a shining tree, a family, I
</span><span class=c1>// can hear the bells even, sleigh bells, from the radio, old music, but through
</span><span class=c1>// this window I can see, small but very clear, I can see her, going away from me,
</span><span class=c1>// through the trees which are already turning, red and yellow, holding out her
</span><span class=c1>// arms to me, being carried away.
</span><span class=c1>//
</span><span class=c1>// The only possible starter would be &#34;She&#39;s&#34;.
</span><span class=c1>// Chain would contain around 80 prefix+candidates pairs:
</span><span class=c1>//
</span><span class=c1>// a       [little very telescope, Christmas candle, shining family,]
</span><span class=c1>// already [turning,]
</span><span class=c1>// an      [old]
</span><span class=c1>// and     [the nothing ice within yellow,]
</span><span class=c1>// apart,  [my]
</span><span class=c1>// are     [held, already]
</span><span class=c1>// ... 
</span><span class=c1>// little  [window, window,]
</span><span class=c1>// ...
</span><span class=c1>// very    [little clear,]
</span><span class=c1>// ...
</span></code></pre></div><p>This means that &ldquo;a&rdquo; may be followed by words &ldquo;little&rdquo;, &ldquo;very&rdquo;, &ldquo;telescope&rdquo; etc. &ldquo;Very&rdquo; may be followed by &ldquo;little&rdquo; or &ldquo;clear&rdquo;. &ldquo;Little&rdquo; may be followed by &ldquo;window&rdquo;.</p><p>Proper Markov chains store probabilities for each candidate word, but we store duplicates of words to keep it simple. This should have the similar effect: in a list of candidates <code>["A", "A", "A", "B"]</code> a random word would have <code>{"A": 0.75, "B": 0.25}</code> probability.</p><p>Training is done. Now, the generator part is also rather straightforward:</p><div class=highlight><pre class=chroma><code class=language-go data-lang=go><span class=nx>w</span> <span class=o>:=</span> <span class=nx>starts</span><span class=p>[</span><span class=nx>rand</span><span class=p>.</span><span class=nf>Intn</span><span class=p>(</span><span class=nb>len</span><span class=p>(</span><span class=nx>starts</span><span class=p>))]</span>
<span class=nx>fmt</span><span class=p>.</span><span class=nf>Print</span><span class=p>(</span><span class=nx>w</span><span class=p>,</span> <span class=s>&#34; &#34;</span><span class=p>)</span>
<span class=k>defer</span> <span class=nx>fmt</span><span class=p>.</span><span class=nf>Println</span><span class=p>()</span>
<span class=k>for</span> <span class=p>{</span>
    <span class=nx>candidates</span> <span class=o>:=</span> <span class=nx>chain</span><span class=p>[</span><span class=nx>w</span><span class=p>]</span>
    <span class=k>if</span> <span class=nb>len</span><span class=p>(</span><span class=nx>candidates</span><span class=p>)</span> <span class=o>==</span> <span class=mi>0</span> <span class=p>{</span>
        <span class=k>break</span>
    <span class=p>}</span>
    <span class=nx>next</span> <span class=o>:=</span> <span class=nx>candidates</span><span class=p>[</span><span class=nx>rand</span><span class=p>.</span><span class=nf>Intn</span><span class=p>(</span><span class=nb>len</span><span class=p>(</span><span class=nx>candidates</span><span class=p>))]</span>
    <span class=nx>fmt</span><span class=p>.</span><span class=nf>Print</span><span class=p>(</span><span class=nx>next</span><span class=p>,</span> <span class=s>&#34; &#34;</span><span class=p>)</span>
    <span class=nx>w</span> <span class=p>=</span> <span class=nx>next</span>
<span class=p>}</span>
</code></pre></div><p>We pick a random starter word, print it, find all candidates, pick one randomly, find candidates for that word, pick one randomly and keep doing so until we find a word with no candidates (indicating the end of a sentence). Such an algorithm produces text like this:</p><div class=highlight><pre class=chroma><code class=language-fallback data-lang=fallback>She’s too young, it’s too late, we come apart, my arms to me, being carried away.
She’s too young, it’s too young, it’s too young, it’s too late, we come apart, my arms are held, and ice outside, and nothing is left but through this window on a shining tree, a Christmas card, an old music, but very clear, I can hear the wrong end of a family, I can hear the radio, old one, night and yellow, holding out her arms are already turning, red and yellow, holding out her arms to me, being carried away.
</code></pre></div><p>For a chain trained on just one sentence it&rsquo;s not too bad. How about training it on a larger text, say all of the <a href=https://www.kaggle.com/code/ambarish/fun-in-text-mining-with-simpsons>Simpsons</a> quotes?</p><div class=highlight><pre class=chroma><code class=language-fallback data-lang=fallback>You coulda been handed down. And one hundred spaceships...
Sure, boy. I don&#39;t have now! No cameras.
Oh, dammit! Run for him! Let&#39;s watch. I-I-I-I was just don&#39;t forget it. Time for the pull the Springfield Elementary. She&#39;s the stomach?
Mr. Simpson? I quit! I have directed three time has eaten an orphanage?
You know, whatever. Just wonderin&#39; if he is secure.
Wow. Mental Patient, Hillbilly, or a terrible trouble. That&#39;s a little boy oh boy. Eww. I killed you. I am so much nicer table pockets, and probed, not thank God! Oh gee it&#39;s a grandmother?
</code></pre></div><p>Not very impressive. But we can make it better if we increase the prefix length. Currently, the chain tracks transitions from one word to another, which often guides it off track. But if we start using two consequent words as a prefix - the text quality should improve significantly.</p><p>Let&rsquo;s make the prefix length variadic and try again:</p><div class=highlight><pre class=chroma><code class=language-go data-lang=go><span class=kd>func</span> <span class=nf>main</span><span class=p>()</span> <span class=p>{</span>
  <span class=nx>n</span> <span class=o>:=</span> <span class=mi>2</span> <span class=c1>// prefix length
</span><span class=c1></span>  <span class=nx>chain</span> <span class=o>:=</span> <span class=kd>map</span><span class=p>[</span><span class=kt>string</span><span class=p>][]</span><span class=kt>string</span><span class=p>{}</span>
  <span class=nx>starts</span> <span class=o>:=</span> <span class=p>[]</span><span class=kt>string</span><span class=p>{}</span>
  <span class=nx>scanner</span> <span class=o>:=</span> <span class=nx>bufio</span><span class=p>.</span><span class=nf>NewScanner</span><span class=p>(</span><span class=nx>os</span><span class=p>.</span><span class=nx>Stdin</span><span class=p>)</span>
    <span class=c1>// Training
</span><span class=c1></span>  <span class=k>for</span> <span class=nx>scanner</span><span class=p>.</span><span class=nf>Scan</span><span class=p>()</span> <span class=p>{</span>
    <span class=nx>words</span> <span class=o>:=</span> <span class=nx>strings</span><span class=p>.</span><span class=nf>Fields</span><span class=p>(</span><span class=nx>scanner</span><span class=p>.</span><span class=nf>Text</span><span class=p>())</span>
    <span class=k>if</span> <span class=nb>len</span><span class=p>(</span><span class=nx>words</span><span class=p>)</span> <span class=p>&lt;</span> <span class=nx>n</span> <span class=p>{</span>
      <span class=k>continue</span>
    <span class=p>}</span>
    <span class=nx>words</span> <span class=p>=</span> <span class=nb>append</span><span class=p>(</span><span class=nx>words</span><span class=p>,</span> <span class=nb>make</span><span class=p>([]</span><span class=kt>string</span><span class=p>,</span> <span class=nx>n</span><span class=p>)</span><span class=o>...</span><span class=p>)</span>
    <span class=nx>starts</span> <span class=p>=</span> <span class=nb>append</span><span class=p>(</span><span class=nx>starts</span><span class=p>,</span> <span class=nx>strings</span><span class=p>.</span><span class=nf>Join</span><span class=p>(</span><span class=nx>words</span><span class=p>[:</span><span class=nx>n</span><span class=p>],</span> <span class=s>&#34; &#34;</span><span class=p>))</span>
    <span class=k>for</span> <span class=nx>i</span> <span class=o>:=</span> <span class=mi>0</span><span class=p>;</span> <span class=nx>i</span> <span class=p>&lt;</span> <span class=nb>len</span><span class=p>(</span><span class=nx>words</span><span class=p>)</span><span class=o>-</span><span class=nx>n</span><span class=o>-</span><span class=mi>1</span><span class=p>;</span> <span class=nx>i</span><span class=o>++</span> <span class=p>{</span>
      <span class=nx>prefix</span> <span class=o>:=</span> <span class=nx>strings</span><span class=p>.</span><span class=nf>Join</span><span class=p>(</span><span class=nx>words</span><span class=p>[</span><span class=nx>i</span><span class=p>:</span><span class=nx>i</span><span class=o>+</span><span class=nx>n</span><span class=p>],</span> <span class=s>&#34; &#34;</span><span class=p>)</span>
      <span class=nx>chain</span><span class=p>[</span><span class=nx>prefix</span><span class=p>]</span> <span class=p>=</span> <span class=nb>append</span><span class=p>(</span><span class=nx>chain</span><span class=p>[</span><span class=nx>prefix</span><span class=p>],</span> <span class=nx>words</span><span class=p>[</span><span class=nx>i</span><span class=o>+</span><span class=nx>n</span><span class=p>])</span>
    <span class=p>}</span>
  <span class=p>}</span>
    <span class=c1>// Generator
</span><span class=c1></span>  <span class=nx>w</span> <span class=o>:=</span> <span class=nx>starts</span><span class=p>[</span><span class=nx>rand</span><span class=p>.</span><span class=nf>Intn</span><span class=p>(</span><span class=nb>len</span><span class=p>(</span><span class=nx>starts</span><span class=p>))]</span>
  <span class=nx>fmt</span><span class=p>.</span><span class=nf>Print</span><span class=p>(</span><span class=nx>w</span><span class=p>,</span> <span class=s>&#34; &#34;</span><span class=p>)</span>
  <span class=k>defer</span> <span class=nx>fmt</span><span class=p>.</span><span class=nf>Println</span><span class=p>()</span>
  <span class=k>for</span> <span class=p>{</span>
    <span class=nx>candidates</span> <span class=o>:=</span> <span class=nx>chain</span><span class=p>[</span><span class=nx>w</span><span class=p>]</span>
    <span class=k>if</span> <span class=nb>len</span><span class=p>(</span><span class=nx>candidates</span><span class=p>)</span> <span class=o>==</span> <span class=mi>0</span> <span class=p>{</span>
      <span class=k>break</span>
    <span class=p>}</span>
    <span class=nx>next</span> <span class=o>:=</span> <span class=nx>candidates</span><span class=p>[</span><span class=nx>rand</span><span class=p>.</span><span class=nf>Intn</span><span class=p>(</span><span class=nb>len</span><span class=p>(</span><span class=nx>candidates</span><span class=p>))]</span>
    <span class=nx>fmt</span><span class=p>.</span><span class=nf>Print</span><span class=p>(</span><span class=nx>next</span><span class=p>,</span> <span class=s>&#34; &#34;</span><span class=p>)</span>
    <span class=nx>w</span> <span class=p>=</span> <span class=nx>strings</span><span class=p>.</span><span class=nf>Join</span><span class=p>(</span><span class=nb>append</span><span class=p>(</span><span class=nx>strings</span><span class=p>.</span><span class=nf>Fields</span><span class=p>(</span><span class=nx>w</span><span class=p>)[</span><span class=mi>1</span><span class=p>:</span><span class=nx>n</span><span class=p>],</span> <span class=nx>next</span><span class=p>),</span> <span class=s>&#34; &#34;</span><span class=p>)</span>
  <span class=p>}</span>
<span class=p>}</span>

</code></pre></div><p>Here&rsquo;s the text produced with different prefix lengths trained on &ldquo;The Simpsons&rdquo; corpus:</p><div class=highlight><pre class=chroma><code class=language-fallback data-lang=fallback>N=1:
&gt; You see, I think a phone. Then how Democrats sold the phonies -- can we lost the following the luck! They couldn&#39;t have a Gamecock!
&gt; Uh, uh, I gotta record for my life. Warmest regards, Love me give a veterinarian and maybe he&#39;ll never drank my ass. I&#39;ll cook dinner? Lasagna would come along with the cartoon. Jack and push some ducklings.
&gt; That&#39;s why did you your expense!

N=2:
&gt; Give me back my belly for four-and-a-half-months. When you call me a bigger ball of yarn. That&#39;s funny, I prefer a dead boyfriend!
&gt; In the last time for this. I&#39;m not dead, idiot.
&gt; Yeah, someone really kill a man like me. My name is Stacy, but you wouldn&#39;t be teachin&#39; there.
&gt; Homer! You promised!

N=3:
&gt; I always keep a Bible close to my Shauna, if someone so much as peels a ladybug decal off her fake fingernails, I&#39;m blaming you.
&gt; I shouldn&#39;t have put so much syrup on the pancakes.
&gt; Now, guess how much of this is true!
</code></pre></div><p>As the length of the prefix grows - the text becomes more meaningful, but also it often repeats the exact phrases from the training text. For English language the length of 2 is considered being a good compromise between creativity and meaningfulness.</p><p>Trying N=2 on a corpus from <a href=https://www.paulgraham.com/articles.html>Paul Graham</a> essays we might get results like this:</p><ul><li>And whereas Wikipedia&rsquo;s main appeal is that it&rsquo;s not a sufficient one.</li><li>You can change anything about how to choose problems as well as being bad in what must have been sent packing by the idea.</li><li>He&rsquo;s not just within existing industries that spiked the sharpest before the underlying principle.</li><li>It was small compared to what he does well and the resulting vacuum will add force to sell early on.</li></ul><p>All of the code from this post can be found on <a href=https://github.com/zserge/aint/tree/main/markov>GitHub</a>.</p><h1 id=ai>AI?</h1><p>Due to its simplicity, Markov chain model is often used in simple applications such as name generators, word prediction for keyboard input, even PageRank algorithm of Google was a variation of a Markov chain.</p><p>Although Markov chains show certain progress compared to Eliza, they are still <a href=https://en.wikipedia.org/wiki/Stochastic_parrot>stochastic parrots</a>. However, the term &ldquo;AI&rdquo; (artificial intelligence) often goes hand in hand with &ldquo;ML&rdquo; (machine learning), and Markov chain is an example of the most basic machine learning algorithm: it can teach itself some knowledge and use it later. In the next part we&rsquo;ll look at some more advanced ML, such as neural networks.</p><p>Next part: <a href=/posts/ai-nn/>Neural Networks</a></p><p>I hope you’ve enjoyed this article. You can follow – and contribute to – on <a href=https://github.com/zserge>Github</a>, <a href=https://mastodon.social/@zserge>Mastodon</a>, <a href=https://twitter.com/zsergo>Twitter</a> or subscribe via <a href=/rss.xml>rss</a>.</p><p class=date style=text-align:right><em>Jan 02, 2024</em></p><p>See also:
<a href=/posts/ai-eliza/>AI or ain't: Eliza</a> and <a href=/posts/>more</a>.</p></div><footer><p>&copy;2012&ndash;2023 &#183;
<a class=h-card rel=me href=https://zserge.com/>Serge Zaitsev</a> &#183;
<a href=mailto:hello@zserge.com rel=me>hello@zserge.com</a> &#183;
<a href=https://mastodon.social/@zserge rel=me>@zserge@mastodon.social</a></p></footer><script>new Image().src='https://nullitics.com/file.gif?u='+encodeURI(location.href)+'&r='+encodeURI(document.referrer)+'&d='+screen.width</script><noscript><img src=https://nullitics.com/file.gif></noscript></body></html>